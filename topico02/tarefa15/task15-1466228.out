Iniciando...
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   r1i3n13
  Local device: mlx4_0
--------------------------------------------------------------------------

| Nº de Processos | Send/Recv (s) | Isend/Irecv+Wait (s) | Isend/Irecv+Test (s) |
|------------------|---------------|------------------------|------------------------|
|                2 |      0.000915 |               0.000906 |               0.000890 |
[r1i3n13.npad.internal:1952013] 1 more process has sent help message help-mpi-btl-openib.txt / error in device init
[r1i3n13.npad.internal:1952013] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   r1i3n13
  Local device: mlx4_0
--------------------------------------------------------------------------

| Nº de Processos | Send/Recv (s) | Isend/Irecv+Wait (s) | Isend/Irecv+Test (s) |
|------------------|---------------|------------------------|------------------------|
|                4 |      0.001565 |               0.001472 |               0.001430 |
[r1i3n13.npad.internal:1952027] 3 more processes have sent help message help-mpi-btl-openib.txt / error in device init
[r1i3n13.npad.internal:1952027] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
WARNING: There is at least one non-excluded one OpenFabrics device found,
but there are no active ports detected (or Open MPI was unable to use
them).  This is most certainly not what you wanted.  Check your
cables, subnet manager configuration, etc.  The openib BTL will be
ignored for this job.

  Local host: r1i3n13
--------------------------------------------------------------------------

| Nº de Processos | Send/Recv (s) | Isend/Irecv+Wait (s) | Isend/Irecv+Test (s) |
|------------------|---------------|------------------------|------------------------|
|                8 |      0.001503 |               0.001348 |               0.001364 |
[r1i3n13.npad.internal:1952051] 7 more processes have sent help message help-mpi-btl-openib.txt / no active ports found
[r1i3n13.npad.internal:1952051] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   r1i3n13
  Local device: mlx4_0
--------------------------------------------------------------------------

| Nº de Processos | Send/Recv (s) | Isend/Irecv+Wait (s) | Isend/Irecv+Test (s) |
|------------------|---------------|------------------------|------------------------|
|               16 |      0.002374 |               0.002159 |               0.002075 |
[r1i3n13.npad.internal:1952095] 15 more processes have sent help message help-mpi-btl-openib.txt / error in device init
[r1i3n13.npad.internal:1952095] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
There are not enough slots available in the system to satisfy the 32
slots that were requested by the application:

  ./main

Either request fewer slots for your application, or make more slots
available for use.

A "slot" is the Open MPI term for an allocatable unit where we can
launch a process.  The number of slots available are defined by the
environment in which Open MPI processes are run:

  1. Hostfile, via "slots=N" clauses (N defaults to number of
     processor cores if not provided)
  2. The --host command line parameter, via a ":N" suffix on the
     hostname (N defaults to 1 if not provided)
  3. Resource manager (e.g., SLURM, PBS/Torque, LSF, etc.)
  4. If none of a hostfile, the --host command line parameter, or an
     RM is present, Open MPI defaults to the number of processor cores

In all the above cases, if you want Open MPI to default to the number
of hardware threads instead of the number of processor cores, use the
--use-hwthread-cpus option.

Alternatively, you can use the --oversubscribe option to ignore the
number of available slots when deciding the number of processes to
launch.
--------------------------------------------------------------------------
Finalizado!
